# Data Modeling with Postgres

## Overview

In this project, we apply Data Modeling with Postgres and build an ETL pipeline using Python. We want to analyze the data collected on songs and user activity on their new music streaming app. Currently, they are collecting data in json format and the analytics team is particularly interested in understanding what songs users are listening to.

The objective of the project are-
1. Data Modelling with Postgres and create a star schema database
2. Setup a ETL pipeline in Python to load data into the tables

## Song Dataset
Songs dataset is a subset of [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.

Sample Record :

> {"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}

## Log Dataset
Logs dataset is generated by an [event simulator](https://github.com/Interana/eventsim) based on the songs in the above dataset. These simulate activity logs from a music streaming app. 

Sample Record :

> {"artist": null, "auth": "Logged In", "firstName": "Walter", "gender": "M", "itemInSession": 0, "lastName": "Frye", "length": null, "level": "free", "location": "San Francisco-Oakland-Hayward, CA", "method": "GET","page": "Home", "registration": 1540919166796.0, "sessionId": 38, "song": null, "status": 200, "ts": 1541105830796, "userAgent": "\"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"", "userId": "39"}

## Schema

### Fact Table
**songplays** - records in log data associated with song plays i.e. records with page NextSong

> songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

### Dimension Tables
**users** - users in the app

> user_id, first_name, last_name, gender, level

**songs** - songs in music database

> song_id, title, artist_id, year, duration

**artists** - artists in music database

> artist_id, name, location, latitude, longitude

**time** - timestamps of records in songplays broken down into specific units

> start_time, hour, day, week, month, year, weekday

## Project Files

**sql_queries.py** -> contains all your sql queries, and is imported into the files below.

**create_tables.py** -> drops and creates tables. You run this file to reset your tables before each time you run your ETL scripts.

**etl.ipynb** -> reads and processes a single file from song_data and log_data and loads the data into your tables.

**etl.py** -> reads and processes files from song_data and log_data and loads them into your tables. Its implemented based on the ETL phylosophy used to processes a single file from song_data and log_data in etl.ipynb.

**test.ipynb** -> a notebook to connect to postgres db and validate the data loaded.

**SQLQueryAnalysis.ipynb** -> a notebook to analyse the database with SQL queries.

**data-folder** -> this folder containes the downloaded files of song dataset and log dataset


## How to Run

- Run the following query in console to create the tables 
> python3 create_tables.py
- Use test.ipynb Jupyter Notebook to interactively verify that all tables were created. Make sure to click "Restart kernel" to close the connection to the database after running this notebook.

- Run the following query in console to create the ETL pipeline using Python
>python3 etl.py 
- Use test.ipynb Jupyter Notebook to interactively verify that records were inserted in the correct tables. Make sure to click "Restart kernel" to close the connection to the database after running this notebook.
- Use SQLQueryExamples.ipynb to see example queries and results for song play analysis. Make sure to click "Restart kernel" to close the connection to the database after running this notebook.
 

## Reference:
[PostgreSQL Documentation](https://www.postgresql.org/docs/)

[Psycopg](https://www.psycopg.org/docs/)

[Pandas Documentation](https://pandas.pydata.org/pandas-docs/stable/)